{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld1NqIA56kmz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Conv2D,MaxPool2D,GlobalMaxPool2D,Flatten,Dense,Dropout,Input,Lambda\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "import keras.backend as K\n",
        "import librosa\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from sklearn.utils import shuffle\n",
        "import cv2\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spectrogram(clip,sample_rate,save_path):\n",
        "  plt.interactive(False)\n",
        "  fig=plt.figure(figsize=[0.72,0.72])\n",
        "  ax=fig.add_subplot(111)\n",
        "  ax.axes.get_xaxis().set_visible(False)\n",
        "  ax.axes.get_yaxis().set_visible(False)\n",
        "  ax.set_frame_on(False)\n",
        "  S=librosa.feature.melspectrogram(y=clip,sr=sample_rate)\n",
        "  librosa.display.specshow(librosa.power_to_db(S,ref=np.max))\n",
        "  fig.savefig(save_path,dpi=400,bbox_inches='tight',pad_inches=0)\n",
        "  plt.close()\n",
        "  fig.clf()\n",
        "  plt.close(fig)\n",
        "  plt.close('all')\n",
        "  del save_path,clip,sample_rate,fig,ax,S"
      ],
      "metadata": {
        "id": "TKO6MaIM6wiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder(input_size):\n",
        "  model=Sequential()\n",
        "  model.add(Conv2D(32,(3,3),input_shape=(150,150,3),activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "  model.add(GlobalMaxPool2D())\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "a7ZR4X8t7C-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_siamese_network(encoder,input_size):\n",
        "  input1=Input(input_size)\n",
        "  input2=Input(input_size)\n",
        "\n",
        "  encoder_l=encoder(input1)\n",
        "  encoder_r=encoder(input2)\n",
        "\n",
        "  # The encoder output is (None, 64), so the difference will also be (None, 64).\n",
        "  # We need to specify the shape without the batch dimension, which is (64,).\n",
        "  L1_layer = Lambda(lambda tensors:tf.math.abs(tensors[0] - tensors[1]), output_shape=(64,))\n",
        "  L1_distance = L1_layer([encoder_l, encoder_r])\n",
        "\n",
        "  output=Dense(1,activation='sigmoid')(L1_distance)\n",
        "  siam_model=Model(inputs=[input1,input2],outputs=output)\n",
        "  return siam_model\n",
        "\n",
        "encoder=get_encoder((150,150,3))\n",
        "siamese_net=get_siamese_network(encoder,(150,150,3))\n",
        "siamese_net.compile(loss='binary_crossentropy',optimizer='adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLArPjPO7N4Y",
        "outputId": "ad714f4a-ace6-465e-f3b0-53635296405f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def different_label_index(X):\n",
        "    idx1=0\n",
        "    idx2=0\n",
        "    while idx1==idx2:\n",
        "        idx1=np.random.randint(0,len(X))\n",
        "        idx2=np.random.randint(0,len(X))\n",
        "    return idx1,idx2\n",
        "\n",
        "def batch_generator(X_data,batch_size):\n",
        "  while True:\n",
        "    data=[np.zeros((batch_size,150,150,3)) for i in range(2)]\n",
        "    tar=[np.zeros(batch_size,)]\n",
        "\n",
        "    #Generating same pairs.\n",
        "    for i in range(0,batch_size//2):\n",
        "      idx1=np.random.randint(0,len(X_data))\n",
        "      img1 = X_data[idx1]\n",
        "\n",
        "      data[0][i,:,:,:]=img1\n",
        "      data[1][i,:,:,:]=img1\n",
        "      tar[0][i]=1\n",
        "\n",
        "    #Generating different pairs.\n",
        "    for k in range(batch_size//2,batch_size):\n",
        "      idx1=np.random.randint(0,len(X_data))\n",
        "      img1 = X_data[idx1]\n",
        "\n",
        "      idx1_diff,idx2_diff=different_label_index(X_data) # Use different_label_index for distinct indices\n",
        "      img2 = X_data[idx2_diff]\n",
        "\n",
        "      data[0][k,:,:,:]=img1\n",
        "      data[1][k,:,:,:]=img2\n",
        "      tar[0][k]=0\n",
        "\n",
        "    yield (data[0], data[1]),tar[0]"
      ],
      "metadata": {
        "id": "ilD9x_Eq7a7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./Spectrograms/', exist_ok=True)\n",
        "songs_path = '/content/Music/'\n",
        "songs_list = [f for f in os.listdir(songs_path) if os.path.isfile(os.path.join(songs_path, f))] # Filter for files only\n",
        "\n",
        "#Read the songs,divide them into 10s segment,create spectrogram of them\n",
        "\n",
        "charsets=string.ascii_letters\n",
        "\n",
        "def get_random_name():\n",
        "    name=''.join([random.choice(charsets) for _ in range(20)])\n",
        "    name=name+str(np.random.randint(0,1000))\n",
        "    return name\n",
        "\n",
        "for song in songs_list:\n",
        "    print(song)\n",
        "    songfile,sr=librosa.load(songs_path+song)\n",
        "    duration=librosa.get_duration(y=songfile,sr=sr)\n",
        "    prev=0\n",
        "    for i in range(1,int((duration//10)+1)):\n",
        "        if i==int((duration//10)):\n",
        "            \"\"\"Since we are dividing the song in 10s segment there might be case that after taking 10\n",
        "            fragments also few more seconds are left so in this case extra becomes extra=extra+(10-extra)\n",
        "            from the previous segment.\"\"\"\n",
        "            extra=int((int(duration)/10-int(int(duration)/10))*10)\n",
        "            st=(sr*i*10)-(10-extra)\n",
        "            end=st+10\n",
        "            songfrag=np.copy(songfile[st:end])\n",
        "        else:\n",
        "            songfrag=np.copy(songfile[prev:(sr*i*10)])\n",
        "        specname=get_random_name()\n",
        "        create_spectrogram(songfrag,sr,'./Spectrograms/'+specname+'.png')\n",
        "        prev=sr*i*10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQm09Fkc8eHo",
        "outputId": "01ce1496-1f18-42b9-8b32-7b018a9cdcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Donell Jones - This Luv [cQUYe18YmSw].mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=10\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maxwell - Fortunate [TRfzaBJhTto].mp3\n",
            "Slow Dancing In A Burning Room (Live in L.A.) [32GZ3suxRn4].mp3\n",
            "Alicia Keys - Un-thinkable (I'm Ready) (Official Video).mp3\n",
            "January 28th [d15cxI5yx5c].mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_spec_files = os.listdir('./Spectrograms/')\n",
        "specfilelist = []\n",
        "for filename in raw_spec_files:\n",
        "    full_path = os.path.join('./Spectrograms/', filename)\n",
        "    if os.path.exists(full_path) and os.path.getsize(full_path) > 0:\n",
        "        specfilelist.append(full_path)\n",
        "\n",
        "specfilelist = shuffle(specfilelist)\n",
        "\n",
        "print(f\"Found {len(specfilelist)} valid spectrogram files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp5PQEKDYskz",
        "outputId": "73ce1425-ed8d-45b4-f6eb-590b7a89e931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 549 valid spectrogram files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af7af1aa",
        "outputId": "33a3af92-57a1-4f82-8e47-3f79e293e57e"
      },
      "source": [
        "def load_and_preprocess_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        print(f\"Warning: Could not load image at {path}\")\n",
        "        return None\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (150, 150))\n",
        "    img = img / 255.0  # Normalize pixel values\n",
        "    return img\n",
        "\n",
        "# Load and preprocess all spectrograms once\n",
        "preprocessed_spectrograms = []\n",
        "for spec_path in specfilelist:\n",
        "    img = load_and_preprocess_image(spec_path)\n",
        "    if img is not None:\n",
        "        preprocessed_spectrograms.append(img)\n",
        "\n",
        "# Convert to numpy array\n",
        "preprocessed_spectrograms = np.array(preprocessed_spectrograms)\n",
        "\n",
        "# Shuffle and split the preprocessed data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_data, X_test_data = train_test_split(preprocessed_spectrograms, test_size=0.25, random_state=42)\n",
        "\n",
        "print(f\"Loaded {len(X_train_data)} training spectrograms and {len(X_test_data)} testing spectrograms.\")\n",
        "\n",
        "# Now, update the training cell to use X_train_data and X_test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not load image at ./Spectrograms/.ipynb_checkpoints\n",
            "Loaded 411 training spectrograms and 137 testing spectrograms.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=10\n",
        "\n",
        "# X_train=specfilelist[0:int(0.75*len(specfilelist))]\n",
        "# X_test=specfilelist[int(0.75*len(specfilelist)):]\n",
        "\n",
        "# Use the preprocessed data directly\n",
        "X_train = X_train_data\n",
        "X_test = X_test_data\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001)\n",
        "mc = ModelCheckpoint('embdmodel.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "history=siamese_net.fit(batch_generator(X_train,batch_size),steps_per_epoch=len(X_train)//batch_size,epochs=50,validation_data=batch_generator(X_test,batch_size),\n",
        "                            validation_steps=len(X_test)//batch_size,callbacks=[es,mc],shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnM_eqbc9Wh6",
        "outputId": "4df2702e-9546-4b9d-8626-2749071ebf04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.7129\n",
            "Epoch 1: val_loss improved from inf to 0.67953, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - loss: 0.7123 - val_loss: 0.6795\n",
            "Epoch 2/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.6398\n",
            "Epoch 2: val_loss improved from 0.67953 to 0.65163, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.6397 - val_loss: 0.6516\n",
            "Epoch 3/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.6487\n",
            "Epoch 3: val_loss improved from 0.65163 to 0.63590, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.6481 - val_loss: 0.6359\n",
            "Epoch 4/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5951\n",
            "Epoch 4: val_loss improved from 0.63590 to 0.59769, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.5945 - val_loss: 0.5977\n",
            "Epoch 5/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.5034\n",
            "Epoch 5: val_loss improved from 0.59769 to 0.52454, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.5033 - val_loss: 0.5245\n",
            "Epoch 6/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.4986\n",
            "Epoch 6: val_loss did not improve from 0.52454\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.4984 - val_loss: 0.5297\n",
            "Epoch 7/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.4592\n",
            "Epoch 7: val_loss improved from 0.52454 to 0.50111, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.4586 - val_loss: 0.5011\n",
            "Epoch 8/50\n",
            "\u001b[1m40/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.4542\n",
            "Epoch 8: val_loss improved from 0.50111 to 0.45733, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.4522 - val_loss: 0.4573\n",
            "Epoch 9/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.4551\n",
            "Epoch 9: val_loss did not improve from 0.45733\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.4547 - val_loss: 0.4608\n",
            "Epoch 10/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.4347\n",
            "Epoch 10: val_loss improved from 0.45733 to 0.44821, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.4340 - val_loss: 0.4482\n",
            "Epoch 11/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.3958\n",
            "Epoch 11: val_loss did not improve from 0.44821\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.3961 - val_loss: 0.4621\n",
            "Epoch 12/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3919\n",
            "Epoch 12: val_loss improved from 0.44821 to 0.41615, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.3917 - val_loss: 0.4161\n",
            "Epoch 13/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3972\n",
            "Epoch 13: val_loss improved from 0.41615 to 0.41271, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.3976 - val_loss: 0.4127\n",
            "Epoch 14/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.4207\n",
            "Epoch 14: val_loss improved from 0.41271 to 0.41102, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.4205 - val_loss: 0.4110\n",
            "Epoch 15/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.3714\n",
            "Epoch 15: val_loss did not improve from 0.41102\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.3715 - val_loss: 0.4208\n",
            "Epoch 16/50\n",
            "\u001b[1m40/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.3462\n",
            "Epoch 16: val_loss improved from 0.41102 to 0.38407, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 0.3468 - val_loss: 0.3841\n",
            "Epoch 17/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3754\n",
            "Epoch 17: val_loss did not improve from 0.38407\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3755 - val_loss: 0.3932\n",
            "Epoch 18/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3282\n",
            "Epoch 18: val_loss did not improve from 0.38407\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 0.3280 - val_loss: 0.3878\n",
            "Epoch 19/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2822\n",
            "Epoch 19: val_loss improved from 0.38407 to 0.37628, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.2832 - val_loss: 0.3763\n",
            "Epoch 20/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3501\n",
            "Epoch 20: val_loss did not improve from 0.37628\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3505 - val_loss: 0.3899\n",
            "Epoch 21/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.2754\n",
            "Epoch 21: val_loss improved from 0.37628 to 0.34383, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.2753 - val_loss: 0.3438\n",
            "Epoch 22/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.5104\n",
            "Epoch 22: val_loss did not improve from 0.34383\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.5088 - val_loss: 0.4092\n",
            "Epoch 23/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2794\n",
            "Epoch 23: val_loss did not improve from 0.34383\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.2802 - val_loss: 0.4020\n",
            "Epoch 24/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.3348\n",
            "Epoch 24: val_loss did not improve from 0.34383\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.3348 - val_loss: 0.3605\n",
            "Epoch 25/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.3083\n",
            "Epoch 25: val_loss did not improve from 0.34383\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.3086 - val_loss: 0.3790\n",
            "Epoch 26/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.2429\n",
            "Epoch 26: val_loss did not improve from 0.34383\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.2439 - val_loss: 0.3962\n",
            "Epoch 27/50\n",
            "\u001b[1m40/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.3100\n",
            "Epoch 27: val_loss improved from 0.34383 to 0.33817, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.3094 - val_loss: 0.3382\n",
            "Epoch 28/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3611\n",
            "Epoch 28: val_loss did not improve from 0.33817\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3606 - val_loss: 0.3541\n",
            "Epoch 29/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3295\n",
            "Epoch 29: val_loss did not improve from 0.33817\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3291 - val_loss: 0.3455\n",
            "Epoch 30/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2997\n",
            "Epoch 30: val_loss did not improve from 0.33817\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.3007 - val_loss: 0.3434\n",
            "Epoch 31/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3063\n",
            "Epoch 31: val_loss did not improve from 0.33817\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.3061 - val_loss: 0.3548\n",
            "Epoch 32/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3099\n",
            "Epoch 32: val_loss improved from 0.33817 to 0.31428, saving model to embdmodel.keras\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.3098 - val_loss: 0.3143\n",
            "Epoch 33/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2900\n",
            "Epoch 33: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.2904 - val_loss: 0.3377\n",
            "Epoch 34/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2898\n",
            "Epoch 34: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.2902 - val_loss: 0.3518\n",
            "Epoch 35/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3046\n",
            "Epoch 35: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3050 - val_loss: 0.3607\n",
            "Epoch 36/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3008\n",
            "Epoch 36: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3015 - val_loss: 0.3759\n",
            "Epoch 37/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3131\n",
            "Epoch 37: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3129 - val_loss: 0.3515\n",
            "Epoch 38/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3528\n",
            "Epoch 38: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3519 - val_loss: 0.3343\n",
            "Epoch 39/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.3310\n",
            "Epoch 39: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.3300 - val_loss: 0.3331\n",
            "Epoch 40/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2275\n",
            "Epoch 40: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.2287 - val_loss: 0.3625\n",
            "Epoch 41/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3735\n",
            "Epoch 41: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3731 - val_loss: 0.3745\n",
            "Epoch 42/50\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3692\n",
            "Epoch 42: val_loss did not improve from 0.31428\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.3686 - val_loss: 0.3530\n",
            "Epoch 42: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84K3WTDKB9SZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}