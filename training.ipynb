{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ld1NqIA56kmz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Conv2D,MaxPool2D,GlobalMaxPool2D,Flatten,Dense,Dropout,Input,Lambda\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "import keras.backend as K\n",
        "import librosa\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from sklearn.utils import shuffle\n",
        "import cv2\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spectrogram(clip,sample_rate,save_path):\n",
        "  plt.interactive(False)\n",
        "  fig=plt.figure(figsize=[0.72,0.72])\n",
        "  ax=fig.add_subplot(111)\n",
        "  ax.axes.get_xaxis().set_visible(False)\n",
        "  ax.axes.get_yaxis().set_visible(False)\n",
        "  ax.set_frame_on(False)\n",
        "  S=librosa.feature.melspectrogram(y=clip,sr=sample_rate)\n",
        "  librosa.display.specshow(librosa.power_to_db(S,ref=np.max))\n",
        "  fig.savefig(save_path,dpi=400,bbox_inches='tight',pad_inches=0)\n",
        "  plt.close()\n",
        "  fig.clf()\n",
        "  plt.close(fig)\n",
        "  plt.close('all')\n",
        "  del save_path,clip,sample_rate,fig,ax,S"
      ],
      "metadata": {
        "id": "TKO6MaIM6wiM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder(input_size):\n",
        "  model=Sequential()\n",
        "  model.add(Conv2D(32,(3,3),input_shape=(150,150,3),activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "  model.add(GlobalMaxPool2D())\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "a7ZR4X8t7C-Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_siamese_network(encoder,input_size):\n",
        "  input1=Input(input_size)\n",
        "  input2=Input(input_size)\n",
        "\n",
        "  encoder_l=encoder(input1)\n",
        "  encoder_r=encoder(input2)\n",
        "\n",
        "  # The encoder output is (None, 64), so the difference will also be (None, 64).\n",
        "  # We need to specify the shape without the batch dimension, which is (64,).\n",
        "  L1_layer = Lambda(lambda tensors:tf.math.abs(tensors[0] - tensors[1]), output_shape=(64,))\n",
        "  L1_distance = L1_layer([encoder_l, encoder_r])\n",
        "\n",
        "  output=Dense(1,activation='sigmoid')(L1_distance)\n",
        "  siam_model=Model(inputs=[input1,input2],outputs=output)\n",
        "  return siam_model\n",
        "\n",
        "encoder=get_encoder((150,150,3))\n",
        "siamese_net=get_siamese_network(encoder,(150,150,3))\n",
        "siamese_net.compile(loss='binary_crossentropy',optimizer='adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLArPjPO7N4Y",
        "outputId": "9ff152a7-dbee-4285-c337-a3b1edc770b6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def different_label_index(X):\n",
        "    idx1=0\n",
        "    idx2=0\n",
        "    while idx1==idx2:\n",
        "        idx1=np.random.randint(0,len(X))\n",
        "        idx2=np.random.randint(0,len(X))\n",
        "    return idx1,idx2\n",
        "def load_img(path):\n",
        "  img=cv2.imread(path)\n",
        "  img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "  img=cv2.resize(img,(150,150))\n",
        "  return img\n",
        "\n",
        "\n",
        "def batch_generator(X,batch_size):\n",
        "  while True:\n",
        "    data=[np.zeros((batch_size,150,150,3)) for i in range(2)]\n",
        "    tar=[np.zeros(batch_size,)]\n",
        "\n",
        "    #Generating same pairs.\n",
        "    for i in range(0,batch_size//2):\n",
        "      idx1=np.random.randint(0,len(X))\n",
        "      img1=load_img(X[idx1])\n",
        "      img1=img1/255\n",
        "\n",
        "      data[0][i,:,:,:]=img1\n",
        "      data[1][i,:,:,:]=img1\n",
        "      tar[0][i]=1\n",
        "\n",
        "    #Generating different pairs.\n",
        "    for k in range(batch_size//2,batch_size):\n",
        "      idx1,idx2=different_label_index(X)\n",
        "      img1=load_img(X[idx1])\n",
        "      img1=img1/255\n",
        "      img2=load_img(X[idx2])\n",
        "      img2=img2/255\n",
        "\n",
        "      data[0][k,:,:,:]=img1\n",
        "      data[1][k,:,:,:]=img2\n",
        "      tar[0][k]=0\n",
        "    # The np.delete calls here are problematic as np.delete returns a new array and does not modify in-place.\n",
        "    # Additionally, if all data points are properly filled, there should be no rows with zero value to delete.\n",
        "    # For now, I'm commenting them out as they are not the cause of the current TypeError, but may need attention later.\n",
        "    # np.delete(data[0],np.where(~data[0].any(axis=1))[0], axis=0)\n",
        "    # np.delete(data[1],np.where(~data[1].any(axis=1))[0], axis=0)\n",
        "    yield (data[0], data[1]),tar[0]"
      ],
      "metadata": {
        "id": "ilD9x_Eq7a7W"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./Spectrograms/', exist_ok=True)\n",
        "songs_list=os.listdir('/content/Music/') #Lists all the files in the folder.\n",
        "\n",
        "#Read the songs,divide them into 10s segment,create spectrogram of them\n",
        "\n",
        "charsets=string.ascii_letters\n",
        "\n",
        "def get_random_name():\n",
        "    name=''.join([random.choice(charsets) for _ in range(20)])\n",
        "    name=name+str(np.random.randint(0,1000))\n",
        "    return name\n",
        "\n",
        "for song in songs_list:\n",
        "    print(song)\n",
        "    songfile,sr=librosa.load('/content/Music/'+song)\n",
        "    duration=librosa.get_duration(y=songfile,sr=sr)\n",
        "    prev=0\n",
        "    for i in range(1,int((duration//10)+1)):\n",
        "        if i==int((duration//10)):\n",
        "            \"\"\"Since we are dividing the song in 10s segment there might be case that after taking 10\n",
        "            fragments also few more seconds are left so in this case extra becomes extra=extra+(10-extra)\n",
        "            from the previous segment.\"\"\"\n",
        "            extra=int((int(duration)/10-int(int(duration)/10))*10)\n",
        "            st=(sr*i*10)-(10-extra)\n",
        "            end=st+10\n",
        "            songfrag=np.copy(songfile[st:end])\n",
        "        else:\n",
        "            songfrag=np.copy(songfile[prev:(sr*i*10)])\n",
        "        specname=get_random_name()\n",
        "        create_spectrogram(songfrag,sr,'./Spectrograms/'+specname+'.png')\n",
        "        prev=sr*i*10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQm09Fkc8eHo",
        "outputId": "fdef6935-3984-4024-ddb2-325bbf7667c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alicia Keys - Un-thinkable (I'm Ready) (Official Video).mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=10\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=10\n",
        "specfilelist=os.listdir('./Spectrograms/')\n",
        "specfilelist=['./Spectrograms/'+filename for filename in specfilelist]\n",
        "specfilelist=shuffle(specfilelist)\n",
        "\n",
        "X_train=specfilelist[0:int(0.75*len(specfilelist))]\n",
        "X_test=specfilelist[int(0.75*len(specfilelist)):]\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001)\n",
        "mc = ModelCheckpoint('embdmodel.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "history=siamese_net.fit(batch_generator(X_train,batch_size),steps_per_epoch=len(X_train)//batch_size,epochs=50,validation_data=batch_generator(X_test,batch_size),\n",
        "                            validation_steps=len(X_test)//batch_size,callbacks=[es,mc],shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnM_eqbc9Wh6",
        "outputId": "2aae9277-14cb-4290-b018-af762b70a4b8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.9517\n",
            "Epoch 1: val_loss improved from inf to 0.69484, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 529ms/step - loss: 0.9300 - val_loss: 0.6948\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.7571\n",
            "Epoch 2: val_loss improved from 0.69484 to 0.69474, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7663 - val_loss: 0.6947\n",
            "Epoch 3/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.7007\n",
            "Epoch 3: val_loss improved from 0.69474 to 0.69379, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.7127 - val_loss: 0.6938\n",
            "Epoch 4/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7197\n",
            "Epoch 4: val_loss improved from 0.69379 to 0.69336, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7154 - val_loss: 0.6934\n",
            "Epoch 5/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.6805\n",
            "Epoch 5: val_loss improved from 0.69336 to 0.69227, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.6929 - val_loss: 0.6923\n",
            "Epoch 6/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.6830\n",
            "Epoch 6: val_loss improved from 0.69227 to 0.69149, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6893 - val_loss: 0.6915\n",
            "Epoch 7/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6702\n",
            "Epoch 7: val_loss did not improve from 0.69149\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.6777 - val_loss: 0.6923\n",
            "Epoch 8/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6521\n",
            "Epoch 8: val_loss did not improve from 0.69149\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.6553 - val_loss: 0.6922\n",
            "Epoch 9/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.7104\n",
            "Epoch 9: val_loss did not improve from 0.69149\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.7047 - val_loss: 0.6918\n",
            "Epoch 10/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.6844\n",
            "Epoch 10: val_loss improved from 0.69149 to 0.69101, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.6782 - val_loss: 0.6910\n",
            "Epoch 11/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.6935\n",
            "Epoch 11: val_loss did not improve from 0.69101\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.6914 - val_loss: 0.6910\n",
            "Epoch 12/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.6790\n",
            "Epoch 12: val_loss improved from 0.69101 to 0.69028, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.6831 - val_loss: 0.6903\n",
            "Epoch 13/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7112\n",
            "Epoch 13: val_loss improved from 0.69028 to 0.68969, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.7061 - val_loss: 0.6897\n",
            "Epoch 14/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.7091\n",
            "Epoch 14: val_loss improved from 0.68969 to 0.68925, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.6960 - val_loss: 0.6893\n",
            "Epoch 15/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6343\n",
            "Epoch 15: val_loss did not improve from 0.68925\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6365 - val_loss: 0.6893\n",
            "Epoch 16/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.6812\n",
            "Epoch 16: val_loss improved from 0.68925 to 0.68884, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.6891 - val_loss: 0.6888\n",
            "Epoch 17/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.6413\n",
            "Epoch 17: val_loss did not improve from 0.68884\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.6402 - val_loss: 0.6890\n",
            "Epoch 18/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.6275\n",
            "Epoch 18: val_loss improved from 0.68884 to 0.68878, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.6299 - val_loss: 0.6888\n",
            "Epoch 19/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.5908\n",
            "Epoch 19: val_loss did not improve from 0.68878\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.6081 - val_loss: 0.6893\n",
            "Epoch 20/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.7098\n",
            "Epoch 20: val_loss did not improve from 0.68878\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.7084 - val_loss: 0.6890\n",
            "Epoch 21/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.6299\n",
            "Epoch 21: val_loss did not improve from 0.68878\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6123 - val_loss: 0.6892\n",
            "Epoch 22/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.5761\n",
            "Epoch 22: val_loss improved from 0.68878 to 0.68846, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.5908 - val_loss: 0.6885\n",
            "Epoch 23/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7082\n",
            "Epoch 23: val_loss improved from 0.68846 to 0.68739, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.6899 - val_loss: 0.6874\n",
            "Epoch 24/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6224\n",
            "Epoch 24: val_loss improved from 0.68739 to 0.68659, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.6338 - val_loss: 0.6866\n",
            "Epoch 25/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.7421\n",
            "Epoch 25: val_loss improved from 0.68659 to 0.68573, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.7262 - val_loss: 0.6857\n",
            "Epoch 26/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7586\n",
            "Epoch 26: val_loss improved from 0.68573 to 0.68556, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.7493 - val_loss: 0.6856\n",
            "Epoch 27/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6317\n",
            "Epoch 27: val_loss improved from 0.68556 to 0.68507, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.6370 - val_loss: 0.6851\n",
            "Epoch 28/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6716\n",
            "Epoch 28: val_loss improved from 0.68507 to 0.68409, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.6818 - val_loss: 0.6841\n",
            "Epoch 29/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.7416\n",
            "Epoch 29: val_loss improved from 0.68409 to 0.68397, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.7273 - val_loss: 0.6840\n",
            "Epoch 30/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7020\n",
            "Epoch 30: val_loss did not improve from 0.68397\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.6796 - val_loss: 0.6846\n",
            "Epoch 31/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.6896\n",
            "Epoch 31: val_loss did not improve from 0.68397\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.6833 - val_loss: 0.6840\n",
            "Epoch 32/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.5786\n",
            "Epoch 32: val_loss improved from 0.68397 to 0.68307, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.5951 - val_loss: 0.6831\n",
            "Epoch 33/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.7062\n",
            "Epoch 33: val_loss improved from 0.68307 to 0.68180, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.6956 - val_loss: 0.6818\n",
            "Epoch 34/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.6872\n",
            "Epoch 34: val_loss improved from 0.68180 to 0.68047, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6912 - val_loss: 0.6805\n",
            "Epoch 35/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.7555\n",
            "Epoch 35: val_loss improved from 0.68047 to 0.67987, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - loss: 0.7487 - val_loss: 0.6799\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.6213\n",
            "Epoch 36: val_loss did not improve from 0.67987\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.6151 - val_loss: 0.6801\n",
            "Epoch 37/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.6746\n",
            "Epoch 37: val_loss improved from 0.67987 to 0.67950, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6787 - val_loss: 0.6795\n",
            "Epoch 38/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.5232\n",
            "Epoch 38: val_loss did not improve from 0.67950\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.5540 - val_loss: 0.6795\n",
            "Epoch 39/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.6455\n",
            "Epoch 39: val_loss improved from 0.67950 to 0.67915, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.6241 - val_loss: 0.6792\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.6914\n",
            "Epoch 40: val_loss improved from 0.67915 to 0.67770, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.6850 - val_loss: 0.6777\n",
            "Epoch 41/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7012\n",
            "Epoch 41: val_loss improved from 0.67770 to 0.67572, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6907 - val_loss: 0.6757\n",
            "Epoch 42/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6926\n",
            "Epoch 42: val_loss improved from 0.67572 to 0.67380, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.6880 - val_loss: 0.6738\n",
            "Epoch 43/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.7037\n",
            "Epoch 43: val_loss improved from 0.67380 to 0.67141, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.6875 - val_loss: 0.6714\n",
            "Epoch 44/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.6217\n",
            "Epoch 44: val_loss improved from 0.67141 to 0.66970, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.6188 - val_loss: 0.6697\n",
            "Epoch 45/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.5621\n",
            "Epoch 45: val_loss improved from 0.66970 to 0.66689, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.5904 - val_loss: 0.6669\n",
            "Epoch 46/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.6835\n",
            "Epoch 46: val_loss improved from 0.66689 to 0.66553, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.6645 - val_loss: 0.6655\n",
            "Epoch 47/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6075\n",
            "Epoch 47: val_loss improved from 0.66553 to 0.66509, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.6085 - val_loss: 0.6651\n",
            "Epoch 48/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.5703\n",
            "Epoch 48: val_loss improved from 0.66509 to 0.66343, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.5758 - val_loss: 0.6634\n",
            "Epoch 49/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.6858\n",
            "Epoch 49: val_loss improved from 0.66343 to 0.66175, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6769 - val_loss: 0.6617\n",
            "Epoch 50/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.6661\n",
            "Epoch 50: val_loss improved from 0.66175 to 0.66045, saving model to embdmodel.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.6414 - val_loss: 0.6605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvoQxFZXAPVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}